{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1. KNN & Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from YourAnswer import naive_softmax_loss, vectorized_softmax_loss, Softmax\n",
    "from YourAnswer import predictKNN\n",
    "from utils import plotData, vis_decision_boundary\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>KNN is a form of <i>instance</i>, or <i>memory</i> based learning wherein we don't learn a function $f(X)$ to estimate $E[Y|X]$. It is a nonlinear, nonparametric model. To make a classification for a given instance ${I}_{i}$, we search the training data for the $k$-nearest neighbors, as defined by some distance metric $d({I}_{i},{I}_{j})$, and take majority vote from $k$ closest points.\n",
    "<br>\n",
    "<br>\n",
    "So, we need a distance metric to determine closest points. The most common distance function used in k-NN is the <i>Euclidean Distance</i>.<br><br>\n",
    "\n",
    "Let ${I} = <I^1,...I^p>$ be a $p$-dimensional vector, then for two instances $i \\text{ and } j$:<br><br>\n",
    "<center>$d_2({I}_{i},{I}_{j}) = \\sqrt{(I_i^{1}-I_j^{1})^2+...+(I_i^{p}-I_j^{p})^2} = \\sqrt{\\sum\\limits_{t=1}^p (I_i^{t}-I_j^{t})^2}$\n",
    "</center>\n",
    "<br><br>\n",
    "\n",
    "See here for more information, \n",
    "(https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"ex1_data1.txt\", header=None, names=['test1', 'test2', 'accepted'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotData(data1)\n",
    "ax.set_ylim([20, 130])\n",
    "ax.legend(['Admitted', 'Not admitted'], loc='best')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1[['test1', 'test2']].values\n",
    "y = data1.accepted.values\n",
    "n, d = X.shape\n",
    "n, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling\n",
    "To obtain the correct results, please fill your code in YourAnswer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "result_knn = predictKNN(X,X,y,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotData(data1)\n",
    "ax.set_ylim([20, 130])\n",
    "i = 0\n",
    "for xy in zip(X[:,0],X[:,1]):\n",
    "    ax.annotate('(%s)' % int(result_knn[i]), xy=xy, textcoords='data',size=8)\n",
    "    i += 1\n",
    "ax.set_ylim([20, 130])\n",
    "ax.legend(['Admitted', 'Not admitted'], loc='best')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('K-nearest neighbors, k = '+str(k)+', training accuracy : ' + str(np.mean(result_knn == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(data1)\n",
    "vis_decision_boundary(X, y, k)\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training accuracy when k becomes large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_from_dif_k = np.zeros((15,))\n",
    "\n",
    "for i, dif_k in enumerate(range(1,30,2)):\n",
    "    result_knn = predictKNN(targetX=X, dataSet=X, labels=y, k=dif_k)\n",
    "    accuracy = np.mean(result_knn == y)\n",
    "    accuracy_from_dif_k[i] = accuracy\n",
    "    \n",
    "#print ('K-nearest neighbors, k = '+str(k)+', training accuracy : ' + str(np.mean(result_knn == y)))    \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(accuracy_from_dif_k, linewidth=2.0)\n",
    "plt.xticks(np.arange(0,15,2),np.arange(1,30,4))\n",
    "plt.ylabel('training accuracy',fontsize=15)\n",
    "plt.xlabel('k',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2. Softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up input preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use CIFAR-10 dataset, which has ten classes. In usual, you need to download it from website, but we have already downloaded and attached it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of CIFAR10 dataset.\n",
    "def load_CIFAR10(root):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(root, 'data_batch_%d' % (b, ))\n",
    "        with open(f, 'rb') as f:\n",
    "            datadict = pickle.load(f, encoding='latin1')\n",
    "            X = datadict['data']\n",
    "            Y = datadict['labels']\n",
    "            X = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "            Y = np.array(Y)\n",
    "        #X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    \n",
    "    f=os.path.join(root, 'test_batch')\n",
    "    with open(f, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Xte = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Yte = np.array(Y)\n",
    "        \n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data():\n",
    "    # 1. Load the raw data\n",
    "    X_tr, Y_tr, X_te, Y_te = load_CIFAR10('./cifar-10-batches-py')\n",
    "    \n",
    "    # 2. Divide the data\n",
    "    X_val, Y_val = X_tr[49000:], Y_tr[49000:]\n",
    "    X_tr, Y_tr = X_tr[:49000], Y_tr[:49000]\n",
    "    X_te, Y_te = X_te[:1000], Y_te[:1000]\n",
    "\n",
    "    # 3. Preprocess the input image\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0],-1))\n",
    "    \n",
    "    # 4. Normalize the data (subtract the mean image)\n",
    "    mean_img = np.mean(X_tr, axis = 0)\n",
    "    X_tr -= mean_img\n",
    "    X_val -= mean_img\n",
    "    X_te -= mean_img\n",
    "\n",
    "    # 5. Add bias and Transform into columns\n",
    "    X_tr = np.hstack([X_tr, np.ones((X_tr.shape[0],1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0],1))])\n",
    "    X_te = np.hstack([X_te, np.ones((X_te.shape[0],1))])\n",
    "    \n",
    "    return X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Validatoin data shape : %s,  Validation labels shape : %s' % (X_val.shape, Y_val.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is successfully loaded into memory, above cell print this message. If not, check the path to directory.\n",
    "\n",
    "    Train data shape : (49000, 3073),  Train labels shape : (49000,)\n",
    "    Validatoin data shape : (1000, 3073),  Validation labels shape : (1000,)\n",
    "    Test data shape : (1000, 3073),  Test labels shape : (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*49000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two version of loss functions for softmax classifier, and test it out on the CIFAR10 dataset.\n",
    "\n",
    "First, implement the naive softmax loss function with nested loops.\n",
    "\n",
    "You should complete naive_softmax_loss function in YourAnswer.py file.\n",
    "\n",
    "Generate a random softmax weight matrix and use it to compute the loss. As a rough sanity check, our loss should be something close to -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = naive_softmax_loss(W, X_tr, Y_tr, 0.0)\n",
    "\n",
    "print ('loss :', loss)\n",
    "print ('sanity check : ', -np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is the vectorized softmax loss function. You can implement this function without loop statements.\n",
    "\n",
    "Compare two versions. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "loss_naive, grad_naive = naive_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('naive loss : %e with %fs' % (loss_naive, time.time()-s_time))\n",
    "\n",
    "s_time = time.time()\n",
    "loss_vectorized, grad_vectorized = vectorized_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('vectorized loss : %e with %fs' % (loss_vectorized, time.time()-s_time))\n",
    "\n",
    "print ('loss difference : %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print ('gradient difference : %f' % np.linalg.norm(grad_naive-grad_vectorized, ord='fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should implement the softmax classifier with softmax loss function you implemented above. Please go to YourFunction.py file and complete Softmax class.\n",
    "\n",
    "Use the validatoin set to tune hyperparameters (regularization strength and learning rate).\n",
    "You should test with different range for the learning rates and regularization strength;\n",
    "With the best choice, you should be able to get the classification accuracy of over 0.34 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results is dictionary mapping tuples of the form.\n",
    "# (learning_rate, regularization_strength) to tuple of the form (training_accuracy, validation_accuracy).\n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-8, 1e-7, 5e-7, 1e-6]\n",
    "regularization_strengths = [5e2, 1e3, 1e4, 5e4]\n",
    "train_acc = 0\n",
    "val_acc = 0\n",
    "#########################################################################################################\n",
    "# TODO : Write code that chooses the best hyperparameters by tuning on the validation set.              # \n",
    "#        For each combination of hyperparemeters, train a Softmax on the training set,                  #\n",
    "#        compute its accuracy on the training and validatoin sets, and store these numbers in the       #\n",
    "#        results dictionary. In addition, store the best validation accuracy in best_val                #\n",
    "#        and the Softmax object that achieves this accuracy in best_softmax.                            #\n",
    "#                                                                                                       #\n",
    "# Hint : You should use a small value for num_iters as you develop your validation code so that the     #\n",
    "#        Softmax don't take much time to train; once you are confident that your validation code works, #\n",
    "#        you should rerun the validation code with a larger value for num_iter.                         #\n",
    "\n",
    "#softmax = Softmax()\n",
    "        \n",
    "for l_rate in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "#------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_tr, Y_tr, X_val, Y_val, l_rate, reg, 200)\n",
    "        train_acc = softmax.get_accuracy(X_tr, Y_tr)\n",
    "        val_acc = softmax.get_accuracy(X_val, Y_val)\n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "#-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "#########################################################################################################\n",
    "        results[(l_rate,reg)] = (train_acc, val_acc)\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy : %f, val accuracy : %f ' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validatoin accuracy achieved during cross-validation :', best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best softmax on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te_pred = best_softmax.predict(X_te)\n",
    "test_accuracy = np.mean(Y_te == Y_te_pred)\n",
    "\n",
    "print ('softmax on raw pixels final test set accuracy : ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize (Image, Predicted label) pairs of the best softmax model. Results may are not good because we train simple softmax classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*1000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_te[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_te_pred[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_softmax.Weights[:-1, :]\n",
    "w = w.reshape(32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    wimg=255.0*(w[:,:,:,i].squeeze() - w_min)/(w_max-w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
